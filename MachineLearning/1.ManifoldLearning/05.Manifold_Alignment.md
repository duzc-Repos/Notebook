可以解决的问题：

* 多组数据集之间的对齐
* 多组数据集之间的表示



## 5.1 Introduction

* fundamental problem: 
  * **aligning** multiple datasets;
  * extract **shared latent structure**; more **meaningful representation**

* Problem formalization:
  * dimensionality reduction with constraints (induced by the correspondences between datasets)

* Two categories:
  * stronger case: a single underlying meaning
    * 在同一空间下找到相同的坐标表示，尽管这个空间不一定代表了"original data", 但是一定程度上反应了数据间的结构
  * weaker case:  related underlying structure

* Algorithmic perspective: 与降维技术高度相关，找到低维流形，并且在低维空间中保留原始空间的结构

### 5.1.1 Problem Statement

* $X\in \mathbb{R}^{n \times p}, Y \in \mathbb{R}^{m \times q}$: two datasets, 
* $Z \in \mathbb{R}^k$: latent space, whose dimensionality is $k$
* $f:\mathbb{R}^p \rightarrow \mathbb{R}^k, g:\mathbb{R}^q \rightarrow \mathbb{R}^k$: the transformation function to be found.
  * 如果$x_i$和$y_i$在$Z$上的geodesic distance比较近，则$f(x_i), g(y_j)$的Euclidean distance也比较近

* $\begin{bmatrix} f(X) \\ g(Y) \end{bmatrix} \in \mathbb{R}^{(m+n) \times k}$: unified representation of $X$ and $Y$



### 5.1.2 Overview of the Algorithm

关键思想：

* 考虑局部关系以及对应关系
* 多个数据集在同一流形上



算法类型：

* supervised: 完全对应信息
* semi-supervised: 不完全对应信息，依赖于已知的对应以及数据集的内在结构建立embedding
* unsupervised: 没有对应信息，需要**推断对应关系**

**重要思想**：

> 所有数据集的Laplacian矩阵都可以看作对同一manifold的离散近似，因此对角堆叠这些Laplacian矩阵，并在非对角填充对应信息同样是该manifold的近似。由此可将多个数据集转换为单个数据集进处理。



## 5.2 Formalization and Analysis

### Notation

* $X^{(a)} \in \mathbb{R}^{n_a \times p_a}$: data matrix
* $W^{(a)} \in \mathbb{R}^{n_a \times n_a}$: 数据$X^{(a)}$的相似矩阵 
* $D^{(a)} \in \mathbb{R}^{n_a \times n_a}$: diagonal matrix, $D^{(a)}(i, i) = \sum\limits_{j=1}^{n_a}W^{(a)}(i, j)$
* $L^{(a)} = D^{(a)} - W^{(a)}$: Laplacian matrix
* $W^{(a, b)} \in \mathbb{R}^{n_a \times n_b}$: 数据集$X^{(a)}$和数据集$X^{(b)}$中实例的对应矩阵，表示两者的相似性 或 对应强度
* $X = \begin{bmatrix} X^{(1)} & \cdots & 0 \\ \vdots & \ddots & \vdots \\ 0 & \cdots & X^{(c)} \end{bmatrix}$ : 数据集对角堆叠
* $W = \begin{bmatrix} \nu W^{(1)} & \mu W^{(1, 2)}& \cdots & \mu W^{(1, c)} \\ \mu W^{(2, 1)}& \nu W^{(2)} & \cdots & \mu W^{(2, c)} \\ \vdots & \vdots & \ddots & \vdots \\ \mu W^{(c, 1)} & \mu W^{(c, 2)} & \cdots & \nu W^{(c)} \end{bmatrix}$: joint similarity matrix
  * $\nu, \mu$ 是scale factor，控制局部相似性和对应信息

* $D, D(i, i) = \sum\limits_{j}W(i, j)$
* $L = D - W$

* $F$: embedding coordinates
  * nonlinear case: $F \in \mathbb{R}^{\left( \sum\limits_{i=1}^c n_i \right) \times d}$ is the new coordinates
  * linear case: $F \in \mathbb{R}^{\left( \sum\limits_{i=1}^c p_i \right) \times d}$, $XF$ is the new coordinates



### 5.2.1 Loss Function

#### Preserving Similarities

* similarity part: $C_\lambda(F^{(a)}) = \sum\limits_{i, j} \parallel F^{(a)}(i, .) - F^{(a)}(j, .)\parallel^2 W^{(a)}(i, j)$

* correspondence part: $C_\kappa(F^{(a)}, F^{(b)}) = \sum\limits_{i, j} \parallel F^{(a)}(i, .) - F^{(b)}(j, .)\parallel^2 W^{(a, b)}(i, j)$

* **complete loss function**:
  $$
  \begin{aligned}
  C_1(F^{(1)}, \cdots, F^{(c)}) & = \nu \sum\limits_{a} C_\lambda(F^{(a)}) + \mu \sum\limits_{a \not= b} C_\kappa(F^{(a)}, F^{(b)}) \\
   &= \nu \sum\limits_{a}\sum\limits_{i, j} \parallel F^{(a)}(i, .) - F^{(a)}(j, .)\parallel^2 W^{(a)}(i, j) + \mu\sum\limits_{a \not= b}\sum\limits_{i, j} \parallel F^{(a)}(i, .) - F^{(b)}(j, .)\parallel^2 W^{(a, b)}(i, j)
  \end{aligned}
  $$



#### Embedding the Joint Laplacian

* 不论两个实例是否来自同一数据集，只要两者相似，那么在latent space中的距离就应该小

$$
\begin{aligned}
C_2(F) &= \sum\limits_{i, j} \parallel F(i, .) - F(j, .) \parallel^2 W(i, j) \\
&= \sum\limits_{i, j} \sum\limits_{k} \parallel F(i, k) - F(j, k) \parallel^2 W(i, j) \\
&= \sum\limits_{k} \sum\limits_{i, j} \parallel F(i, k) - F(j, k) \parallel^2 W(i, j) \\
&= \sum\limits_{k} \tr(F(., k)^TLF(., k)) \\
&= \tr(F^TLF)
\end{aligned}
$$

#### Equivalence

$$
C_1(F^{(1)}, \cdots, F^{(c)}) = C_2(F) = C(F)
$$

#### Optimization Problem

$$
\begin{aligned}
\arg\min\limits_{F} C(F) &= \tr(F^TLF) \\
s.t. & \ \ \ \ F^TDF = I
\end{aligned}
$$

* 对loss的最优值求解需要加入约束，否则直接让$F=0$即可，但这个解是没有意义的
* 加入的约束不同，会产生不同的解释，常见的约束有$F^{T} DF = I, F^TF=I, F^T\mathbf{diag}(D^{(1)}, \cdots, D^{(c)})F$



#### Optimal Solution

* consider $d = 1$, then
  $$
  \arg\min\limits_{f: f^TDf=1} C(f) = \arg\min\limits_{f} f^TLf + \lambda(1 - f^TDf)
  $$

* Differentiating with respect to $f$ and $\lambda$ and setting equal to zero gives
  $$
  \begin{cases}
  Lf-\lambda Df = 0 \\
  f^TDf = 1
  \end{cases}
  $$

* 因此该优化问题是一个广义特征值求解问题



### 5.2.3 Algorithm

> 1. Calculate the adjacent matrices $W^{(1)}, \cdots, W^{(c)}$
> 2. Construct the joint Laplacian $L$
> 3. Compute the $d$ smallest nonzero generalized eigenvectors of $Lf = \lambda Df$
> 4. $F = [f_1, f_2, \cdots, f_d]$



## 5.3 Important variants of manifold alignment

### 5.3.1 Linear restriction

$$
\begin{aligned}
\arg\min\limits_{F} C(F) =& \sum\limits_{i, j} \parallel X(i, .)F - X(j, .)F \parallel^2 W(i, j) = \tr(F^TX^TLXF) \\
s.t\ \ \  & F^TX^XDXF = I   
\end{aligned}
$$

* 同样是广义特征值求解问题，$X^TLXf = \lambda X^TDX f$

* 与Nonlinear alignment比较：
  * 变换有显示表达
  * $F^{(g)}(F^{(h)})^+$可以将g数据集映射到h数据集
  * 如果$\sum\limits_{i=1}^c p_i << \sum\limits_{i=1}^c n_i$， 那么运算速度会更快
  * 当$X$为单位阵时，linear变为nonlinear
  * 可以当作joint feature selection algorithm, $F$提供了关于feature重要性的信息
  * $F^{(g)}(F^{(h)})^+$可以当作是feature-level alignment:



### 5.3.2 Hard constraints

* 如果两组数据中存在样本有明确的对应关系，则可以对$L$矩阵进行修改，添加联合实例，去除原实例对应的行。



### 5.3.3 Multi-scale alignment

* 利用**diffusion wavelets**产生hierarchical solution, 保证局部几何结构并且保证在各个层次上的对应关系。
* 通过探索内在的共同结构，自动产生在**不同尺度的比对结果**（多个空间下的多个结果）。



#### Problem Statement

* $d_1 > d_2 > \cdots > d_h$: a sequence of dimensions
* $X, Y$: two datasets with partial correspondence information
* $\mathcal{A}_k, \mathcal{B}_k, k=1, \cdots, h$: mapping functions
  * projecting samples to a new space
  * preserving local geometry of each datasets
  * matching instances in correspondence
  * ${\rm span}({\cal A}_1) \supseteq {\rm span}({\cal A}_2) \supseteq \cdots \supseteq {\rm span}({\cal A}_h)$，${\rm span}({\cal B}_1) \supseteq {\rm span}({\cal B}_2) \supseteq \cdots \supseteq {\rm span}({\cal B}_h)$
* 



#### Optimal Solutions

* the property of **diffusion wavelets**



### 5.3.4 without correspondence information (Unsupervised)













