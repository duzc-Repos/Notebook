# A Neural Probabilistic Language Model



## 1. Introduction

* language modeling 的主要困难：curse of dimensionality（离散随机变量更明显）
  * 假设有一个词库$V$，其包含的词汇量$|V| = 1000000$。
  * 我们希望对 $n = 10$ 个连续的词(words)的联合分布(joint distribution)建模，那么就会有$|V|^n-1 = 10^{50} - 1$个自由参数

* Generalizaition：
  * 连续变量：由于需学习到的分布自带一些局部连续的性质，泛化（generalization）更容易
  * 离散变量：每一个变量的变化都会导致整体发生巨大变化；如果变量的取值过多

* statistical model of language: ${\rm P}(w_1^T) = \prod\limits_{t=1}^T {\rm P}(w_t \mid w_1^{t-1})$
  * $w_t$: $t$-$th$ word
  * $w_i^{t-1} = [w_i, \cdots, w_j]$: a sequence of words

* n-gram: ${\rm P}(w_t \mid w_1^{t-1}) \approx {\rm P}(w_t \mid w_{t-(n-1)}^{t-1})$
  * 利用最近的前$n-1$个词近似条件概率分布
  * 相当于做了一个$n$阶markov假设
* **n-gram的缺陷**：
  * $n$不能太大，超过$n$个词的信息都不考虑
  * 没有考虑词汇之间的"相似性": 这里的相似主要指语境的相似



### 1.1 Idea of proposed approach

> 1. 为词汇表中的每一个词分配一个词特征向量（word feature vector）
> 2. 用词特征向量表示联合概率分布
> 3. 同时学习词特征向量以及概率分布的参数

* 把每个词映射到一个向量空间：
  * 相比于词库的大小$|V|$，这个词的特征的维度可以很小, e.g. $m=30, 60, 100$
  * 在向量空间中，点之间是可以度量的，一定程度上反映了词之间的相似性
  * 即使$n$很大，也可以用神经网络进行训练



## 2. A Neural Model

![image-20220420120752544](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20220420120752544.png)

### 2.1 Notation

* $V$: 词库，$|V|$表示词库的大小
* $w_t \in V$: 一个词序列中，第t个位置的单词
* A mapping $C: V \rightarrow \mathbb{R}^m$，实际应用中$C \in \mathbb{R}^{|V| \times m}$，可以当作$|V| \times m$个参数
* A mapping $g: \mathbb{R}^{(n-1)m} \rightarrow \mathbb{R}^{|V|}$，其第$i$个分量表示${\rm P}(w_t = i \mid w_1^{t-1})$，本文中使用的是神经网络
* $\theta = (C, \Omega)$: 表示参数集合



### 2.3 Network structure

* 本文中使用的是三层网络：输入层、隐藏层、输出层各一个
* 输入层：
  * 输入：$n-1$个词
  * 输出：$m(n-1)$维的向量
  * 过程：利用映射$C$把每个词映射成$m$维的向量，再对进行拼接，即$x = [C(w_{t-1}), \cdots, C(w_{t-n+1})]$
* 隐藏层
  * 输入:  $x \in \mathbb{R}^{m(n-1)}$
  * 输出: $h$维向量
  * 过程：$o = Hx + d, a = \tanh(o), o, a\in \mathbb{R}^h, H \in \mathbb{R}^{h \times m(n-1)}, d \in \mathbb{R}^h$,
* 输出层
  * 输入： $x \in \mathbb{R}^{m(n-1)}, H \in \mathbb{R}^{h}$
  * 输出：$|V|$维向量，每一元素表示unnormalized log-probabilities
  * 过程：$y = b + Wx + Ua, U \in \mathbb{R}^{|V| \times h}, W \in \mathbb{R}^{|V| \times m(n-1)}, b \in \mathbb{R}^{|V|}$
    * 如果输入层和输出层没有链接，则令$W = 0$即可
* 损失函数：softmax + weight decay penalty
  * ${\rm P}(w_t \mid w_{t-n+1}^{t-1}) = {\rm softmax}(y) = [p_1, \cdots, p_{|V|}]$
    * **此处可能会有溢出问题，可以先对y做处理，同一减去$y$中的最大值**，即$p_j = \exp(y_j - \max y)$
  * $L = \log {\rm softmax}(y)[w_t]$
* summary:
  * $y = b + Wx + U\tanh(d + Hx)$
  * 参数：$\theta = [b, W, U, d, H, C]$
    * $b \in \mathbb{R}^{|V|}$
    * $W \in \mathbb{R}^{|V| \times m(n-1)}$
    * $U \in \mathbb{R}^{|V| \times h}$
    * $d \in \mathbb{R}^{h}$
    * $H \in \mathbb{R}^{h \times m(n-1)}$
    * $C \in \mathbb{R}^{|V| \times m}$
    * 总共的参数个数为$|V|(1 + mn + h) + h(1+m(m-1))$，主要部分是$|V|(nm+h)$



### 2.4 backward/update

* clear $\frac{\part L}{\part a}, \frac{\part L}{\part x}$

* 输出层：
  * $\frac{\part L}{\part y_j} \leftarrow \delta(j==w_t) - p_j$
  * $b_j$: $\frac{\part L}{\part b_j} = \frac{\part L}{\part y_j} \frac{\part y_j}{\part b_j} = \frac{\part L}{\part y_j} \times 1, \Longrightarrow b_j \leftarrow b_j + \epsilon \frac{\part L}{\part y_j}$
  * $U_j$: $\frac{\part L}{\part U_j} = \frac{\part L}{\part y_j} \frac{\part y_j}{\part U_j} = \frac{\part L}{\part y_j} * a^T \Longrightarrow U_j \leftarrow U_j + \epsilon \frac{\part L}{\part y_j} * a^T$
  * 之后使用：$\frac{\part L}{\part o} = \frac{\part L}{\part a} \frac{\part a}{\part o}$
    * $a$: $\frac{\part L}{\part a} = \frac{\part L}{\part y} \frac{\part y}{\part a} = \frac{\part L}{\part y} U$: 隐藏层使用
    * $o$: $\frac{\part a}{ \part o} = \mathbb{1} - a^2$



* 隐藏层：
  * $d$: $\frac{\part L}{\part d} = \frac{\part L}{\part o}$
  * $H$: $\frac{\part L}{\part H_K} = \frac{\part L}{\part a} \frac{\part a}{ \part o} \frac{\part o}{\part H_k}  \Longrightarrow H \leftarrow H + \epsilon \frac{\part L}{\part o} x^T$
  * 之后使用：$\frac{\part L}{\part x} = \frac{\part L}{\part o} \frac{\part o}{\part x} = H^T \frac{\part L}{\part o}$



* 输入层：
  * $C(w_{t-k})$: $\frac{\part L}{\part C(w_{t-k})} = \frac{\part L}{\part x_k} = \frac{\part L}{\part x}[k] \Longrightarrow C(w_{t-k}) \leftarrow C(w_{t-k}) + \epsilon \frac{\part L}{\part w_{t-k}}$



## 4. Results

* Brown corpus: a stream of 1181041 words, 47578 different words, reduce to 16383 words by merge frequency <= 3
  * traning: first 800,000 words
  * validation: following 200,000 words
  * test: remainning 181041 words



### 4.1 N-Gram Models: benchmark

* an interpolated or smoothed trigram model

$$
{\rm P}(w_t \mid w_{t-1}, w_{t-2}) = \alpha_0(q_t) p_0 + \alpha_1(q_t)p_2(w_t \mid w_{t-1}) + \alpha_2(q_t)p_2(w_t \mid w_{t-1}) + \alpha_3(q_t)p_3(w_t \mid w_{t-1}, w_{t-2})
$$

* Notaion
  * $q_t$: 输入文本$w_{t-1}, w_{t-2}$的dicretized frequency, $q_t = l({\rm freq}(w_{t-1}, w_{t-2}))$
    * $l(x) = \lceil -\log((1+x)/T) \rceil$, $T$表示训练集的大小
    * ${\rm freq}(w_{t-1}, w_{t-2})$： 训练集中$w_{t-1}, w_{t-2}$的出现frequency
  * $\alpha_i(q_t)$: conditional weights, $\alpha_i(q_t) \geq 0, \sum\limits_{i=1}^3\alpha_i(q_t) = 1$
  * $p_0 = \frac{1}{|V|}$: base predictors
  * $p_1(i)$: unigram, word i 在训练集中的相对频率
  * $p_2(i \mid j)$: bigram, word i 出现在word j之前的相对频率
  * $p_3(i \mid j, k)$: trigram, 

* Motivation
  * 当$(w_{t-1}, w_{t-2})$的frequency非常大的时候，$p_3$更可靠
  * 当$(w_{t-1}, w_{t-2})$的frequency非常小的时候，$p_2$或$p_0$更可靠
  * 对于不同的$q_t$,就会产生不同的权重$\alpha$
  * 很容易使用EM算法进行估计（5 iterations）





















